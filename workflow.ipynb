{"cells":[{"cell_type":"markdown","metadata":{},"source":["# PyTorch Workflow\n","\n","The typical` DL/ML workflow in PyTorch involves:\n","\n","- Preparing and loading collected data\n","- Building models\n","- Fitting/Training the model on the data\n","- Making Predictions aka. Inference\n","- Saving and reloading trained models\n","- Putting it all together"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","from torch import nn # nn contains all of PyTorch's building blocks for neural networks\n","import matplotlib.pyplot as plt\n","import time\n","\n","from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","# Check for PyTorch version\n","torch.__version__\n","\n","# Setting up a device agnostic code\n","if torch.cuda.is_available():\n","  torch.set_default_device(\"cuda\")\n","else:\n","  torch.set_default_device(\"cpu\")"]},{"cell_type":"markdown","metadata":{},"source":["## Data (Preparing and Loading)\n","\n","Machine Learning is a game of two parts:\n","- Get data into numerical representation\n","- Build a model to learn patterns in that data\n","\n","To showcase this, lets create some known data using linear regression formula. We will make a straight line with known **parameters**.\n","\n","> While creating models we come across the concept of features and labels. Features are the characteristics that the model will evaluate from the training data input and labels are the entities or names or values the model is trained to associate the features to. For example, a CNN trained on a dataset of celebrity faces will have the image of their faces as the feature data and the name of the celebrity will be the feature's label. \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Establishing known parameters\n","weight = 0.7 # eight is the coefficient of the weighted sum\n","bias = 0.3 # bias is an additional constant to be added to the weighted sum to threshold the activation\n","start = 0\n","end = 1\n","step = .02\n","X = torch.arange(start, end, step).unsqueeze(dim=1)\n","# We created a range of numbers from 0 to 1 with steps of 0.02 and then wrapped each number in another dimension so the dimension of X will be ([50,1])\n","Y = weight * X + bias\n","# We map each element of X to Y with a function\n","# print(f\"x:{X[:10]}\\ny:{Y[:10]}\")\n","# print(len(X), len(Y), X.shape, Y.shape)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Splitting data into learning and testing sets\n","\n","One of the most important concepts in machine learning is dividing our data into a set that we learn the patterns from and a set on which we validate our model.\n","\n","There are three types of datasets in machine learning:\n","- The training set\n","- The validation set\n","- The test set\n","\n","This is done to generalize our models(make sure it is able to work on datasets that it has never seen before.)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Creating a training and testing set of our data\n","\n","train_split = int(.8 * len(X)) \n","# We index with integers so it is crucial to convert it into an integer given that the value is implicitly typecast into a float \n","train_split\n","\n","X_train, Y_train = X[:train_split], Y[:train_split]\n","X_test, Y_test = X[train_split:], Y[train_split:]"]},{"cell_type":"markdown","metadata":{},"source":["A better way to understand our data is to visualize them"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_predictions(train_data = X_train.to(\"cpu\"),\n","                     train_labels = Y_train.to(\"cpu\"), \n","                     test_data = X_test.to(\"cpu\"), \n","                     test_labels = Y_test.to(\"cpu\"), \n","                     predictions = None):\n","  \"\"\"\n","  Plots the training data, test data and compares the predictions\n","  \"\"\"\n","  plt.figure(figsize=(10,7)) # Figure dimensions in inches\n","\n","  # Plotting the training data in blue\n","  plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training Data\") # (<data-x>, <data-y>, <color>, <scale>, <label>)\n","\n","  # Plotting the test data in green\n","  plt.scatter(test_data, test_labels, c=\"orange\", s=4, label=\"Test Data\")\n","\n","  # Checking if any predictions have been made\n","  if predictions is not None:\n","    # Plot the predictions\n","    plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n","\n","  # Displaying the legend\n","  plt.legend(prop={\"size\" : 14}); # (<property dictionary>)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_predictions()"]},{"cell_type":"markdown","metadata":{},"source":["Now let's build a model that is able to predict the function based on the training set for the test set."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Creating a linear regression model class\n","\n","class LinearRegressionModel(nn.Module): # Almost everything in PyTorch inherits nn.Module\n","  def __init__(self):\n","    super().__init__() # We initialize the parent class by referring to it as super\n","    self.weight = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n","    self.bias = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n","\n","  # Defining a forward function to represent the computation in model\n","  def forward(self, x: torch.Tensor) -> torch.Tensor:\n","     # We did a little bit of type hinting here, we'll dive into it better below. However, x is the input data here\n","    return self.weight * x + self.bias # We returned the value of the function as per the current model's state (weight, bias)\n","  "]},{"cell_type":"markdown","metadata":{},"source":["A couple of things to bring to light from the above code block:\n","\n","* the `super` method calls the parent class and here we are initializing the parent nn.Module superclass\n","* the `requires_grad` argument asks PyTorch to keep track of gradients of the parameters as we will employ `gradient descent` and `back propagation` to update our parameters\n","* while defining the forward method we employed 'type hinting' which causes the method to expect a tensor as input and the `->` indicated that the method will return a tensor as well\n","* any subclass of nn.Module must override the forward method ( overridig is done by defining the method in the subclass again )\n","\n","> What does the model do?\n","\n","It:\n","* starts with random values for the parameters\n","* looks at the training data and adjust the random values to better suit or represent the ideal values\n","\n","> How does it do so?\n","\n","Through:\n","* Gradient Descent\n","* Back Propagation\n"]},{"cell_type":"markdown","metadata":{},"source":["## PyTorch Model Building Essentials\n","\n","* torch.nn - contains all of the building blocks for computational graphs ( a neural network can be considered a computational graph)\n","* torch.nn.Parameters - what parameters should our model try to learn, often a PyTorch layer from torch.nn will set these for us\n","* torch.nn.Module - the base class for all neural network modules, don't forget to overwrite the forward method. The forward method defines wha happens in forward computation\n","* torch.optim - this is where all of the optimizers of PyTorch live, they determine the best values for our parameters via gradient descent\n"]},{"cell_type":"markdown","metadata":{},"source":["Let's see what's inside our model. The model parameters can be fetched with .parameters() method."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Creating a random seed\n","torch.manual_seed(42)\n","\n","# Creating an instance of the model\n","model_0 = LinearRegressionModel()\n","list(model_0.parameters())"]},{"cell_type":"markdown","metadata":{},"source":["A better way to list out our parameters is through the .state_dict() method. As the name suggests, the method returns a dictionary containing current state information of the model as in the parameter values and so on."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_0.state_dict()"]},{"cell_type":"markdown","metadata":{},"source":["## Making predictions with `torch.inference_mode()`\n","\n","When we pass our data through a model, it is going to run it through the forward model. How well the model is able to guess the value of \"Y_test\" as per the provided \"X_test\" determines the predictive power / accuracy of our model."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# We seen an application of context managers in python below. Context managers are used for resource management where resources no longer in use are automatically released by the program to accomodate for further resources.\n","with torch.inference_mode():\n","  y_preds_rand = model_0(X_test)\n","\n","# We can have achieve similar results with the torch.no_grad() in the context manager however torch.inference_mode() has several benefits over no_grad() being a newer feature of PyTorch\n","# with torch.no_grad():\n","#   ypreds = model_0(X_test)\n","\n","y_preds_rand\n","plot_predictions(predictions=y_preds_rand.to(\"cpu\"))"]},{"cell_type":"markdown","metadata":{},"source":["## Training our Model\n","\n","The entire process of training revolves around starting with some arbitrary parameters (maybe random, maybe taken from some other models `transfer learning`), looking at the data we have, trying to figure out the underlying trend, rule, pattern etc. and changing our parameters to better represent the data we have. The model progressively gets better at predicting the output for an input as per the data's trend.\n","\n","Quantifying the performance of our model allows for us to fine-tune it to be more accurate. One of the ways of doing so is through a `loss function`. Loss functions differ from cost functions in the way that cost functions find out how bad the model is, on average, at predicting the data for several sample inputs while a loss function evaluates how far a model's prediction is from the actual data (ideal prediction) for a single sample input.\n","\n","Things we will need fot training our model:\n","\n","  * **Loss Function** - Described above\n","  * **Optimizer** - Takes into account the loss of the prediction and adjusts the parameters such that the loss is minimized\n","\n","And specifically for PyTorch we will need:\n","  * A training loop\n","  * A testing loop"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Setting up a loss function\n","loss_fn = nn.L1Loss() # Initializes the MAE (Mean Average Error) loss function\n","\n","# Setting up a optimizer\n","optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.01)  # Initializes  the SGD (Stochastic Gradient Descent) optimizer\n"]},{"cell_type":"markdown","metadata":{},"source":["While setting up our loss function, there are a few things to keep in mind\n","* **params** : the parameters that are to be optimized through training\n","* **lr** : the learning rate, one of the most important hyperparameters. Hyperparameters are set by engineers to allow for faster convergence (completion of learning or achievement of local minimum in cost-parameters hyperspace). Smaller lr can cause overfitting of data. Larger parameters can cause overshooting minimum due to large steps.\n","\n","> Overfitting of data is a condition where the model learns the noise and idiosyncrasies of the training data  as if they were genuinely contributing to the underlying pattern, like a student that meticulously memorizes the concepts and notes without gaining the underlying understanding required to solve problems not previously discussed. The model will perform surprisingly well with samples from training data but are severely compromised when faced with new datasets."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["loss_fn\n","optimizer "]},{"cell_type":"markdown","metadata":{},"source":["### Building a training loop and testing loop\n","\n","Thing we will need:\n","0. Loop through the data\n","1. Forward pass (passing the data through forward method of our model) to make predictions- aka Forward Propagation\n","2. Calculate Loss for the prediction (compare prediction with ground-truth labels)\n","3. Optimizer zero grad\n","4. Loss Backwards - to calculate the gradient of each parameters of the model with respect to loss (**backpropagation**)\n","5. Optimizer step - adjust our model parameters to improve our loss (**gradient dscent**)\n"]},{"cell_type":"markdown","metadata":{},"source":["> **Training Loo**p"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["epochs = 100 # An epoch is a loop through data\n","# 0. Loop through the data\n","\n","for i in range(epochs):\n","  # Set the model in training mode\n","  model_0.train() # Sets all parameters in the model that require gradients to require gradients\n","\n","  # 1. Forward pass\n","  Y_preds = model_0(X_train)\n","\n","  # 2. Calculate the loss \n","  loss = loss_fn(Y_preds, Y_train)\n","  \n","  # 3. Optimizer zero grad (see last comment in cell)\n","  optimizer.zero_grad()\n","\n","  # 4. Perform backpropagation on the loss with respect to the parameters of the model\n","  loss.backward()\n","\n","  # 5. Step the optimizer (perform gradient descent)\n","  optimizer.step() \n","  # The optimizer steps will accumulate as we go through the loop. We can get similar idea from the fact that in back propagation the nudge to the parameters be it weight or bias depends not only on the sample training data evaluated but for all data in the set (true backprop). So in order to zero thee optimizer step we pass the statement in 3\n","\n","  model_0.eval() #Turns off model settings not required during testing / evaluation (batch norm, dropout etc.)\n","  with torch.inference_mode():  # Turns off gradient tracking && a couple more things bts.\n","    # 1. Do a forward pass\n","    y_predictions = model_0(X_test)\n","    # 2. Calculate loss\n","    loss_on_prediction = loss_fn(y_predictions, Y_test)\n","\n","  # Print out the metrics\n","  if i % 10 == 0:\n","    print(f\"Epoch: {i}\\n Loss: {loss} | Test Loss: {loss_on_prediction}\")\n","    \n","plot_predictions(predictions=y_predictions.to(\"cpu\"))\n","  \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Refactoring the above cells into a function (recommended)\n","def model_test(model, labels, features, loss_function, i):\n","  model.eval()\n","  with torch.inference_mode():\n","    y_prediction = model(features)\n","    loss_test = loss_function(y_prediction, labels)\n","  if i % 10 == 0:\n","    print(f\"Test Loss: {loss_test}\")\n","\n","def model_train(model, labels, features, epochs, loss_function, optimizer, test_labels, test_features):\n","  start = time.time()\n","  for i in range(epochs):\n","    model.train()\n","    predictions = model(features)\n","    loss = loss_function(predictions,labels)\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","    if i % 10 == 0:\n","      print(f\"Epoch: {i}\\nWeight: {model.weight.item()} | Bias: {model.bias.item()} | Loss: {loss}\")\n","    model_test(model,test_labels, test_features, loss_function, i)\n","  end = time.time()\n","  print(f\"Time Elapsed: {end - start}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_train(model_0, Y_train, X_train, 100, loss_fn, optimizer, Y_test, X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with torch.inference_mode():\n","  plot_predictions(predictions=model_0(Y_test).to(\"cpu\"))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":2}
