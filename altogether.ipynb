{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Putting everything together\n"]},{"cell_type":"markdown","metadata":{},"source":["### 0.1. Importing pytorch"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","from torch import nn\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import time\n","from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","is_in_colab = input(\"Is the model being trained in a colab environment(y/n)?\\n:\")\n","if (is_in_colab.lower()==\"y\"):\n","  import google.colab\n","print(f\"Imported Torch v{torch.__version__}\")"]},{"cell_type":"markdown","metadata":{},"source":["### 0.2. Device Agnostic code\n","\n","The major advantage of using a system equipped with a GPU is the ability of Tensors to be stored in the gpu and operated on by them. GPUs have large number of cored and albeit low power, can crunch to large amount of data rapidly. But it is not always guaranteed that the system where we are training our data is equipped a gpu. In that case we setup a device agnostic code that sets the device of our tensors as per the availability. \n","\n","While it is suggested to train our model in a system that is equipped with a dedicated gpu (usually CUDA cores). We may have to test our model's trainability or framework before it send it to get trained. In such situations running a preliminary training run on small amount of data to determine the efficiency can eliminate the need to attempt long failed runs repeatedly."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Setting up device agnostic code\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"]},{"cell_type":"markdown","metadata":{},"source":["## 1. Data"]},{"cell_type":"markdown","metadata":{},"source":["### 1.1. Preparing data for training and testing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Preparing arbitrary parameters\n","weight = 0.6 \n","bias = 0.2\n","start = 0\n","end = 1\n","step = 0.01\n","X = torch.arange(start, end, step).unsqueeze(dim=1)\n","# Not unsqueezing can cause errors\n","Y = weight * X + bias"]},{"cell_type":"markdown","metadata":{},"source":["### 1.2. Splitting our data into training and testing sets"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_split = int(0.6*len(X))\n","train_val_split = int(0.8*len(X))\n","X_train = X[:train_split]\n","X_test = X[train_val_split:]\n","Y_train = Y[:train_split]\n","Y_test = Y[train_val_split:]\n","X_val = X[train_split:train_val_split]\n","Y_val = Y[train_split:train_val_split]"]},{"cell_type":"markdown","metadata":{},"source":["### 1.3. Visualization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_predictions(train_data = X_train.to(\"cpu\"),\n","                     train_labels = Y_train.to(\"cpu\"),\n","                     val_data = X_val.to(\"cpu\"),\n","                     val_labels = Y_val.to(\"cpu\") ,\n","                     test_data = X_test.to(\"cpu\"), \n","                     test_labels = Y_test.to(\"cpu\"), \n","                     predictions = None):\n","  plt.figure(figsize=(10,7)) # Figure dimensions in inches\n","\n","  # (<data-x>, <data-y>, <color>, <scale>, <label>)\n","  plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training Data\") \n","  plt.scatter(val_data, val_labels, c = \"r\", s=4, label=\"Validation Data\")\n","  plt.scatter(test_data, test_labels, c=\"orange\", s=4, label=\"Test Data\")\n","\n","  if predictions is not None:\n","    # Plot the predictions\n","    plt.scatter(test_data, predictions, c=\"black\", s=4, label=\"Predictions\")\n","\n","  plt.legend(prop={\"size\" : 14}); # (<property dictionary>)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_predictions()"]},{"cell_type":"markdown","metadata":{},"source":["## 2. Building the Model"]},{"cell_type":"markdown","metadata":{},"source":["### 2.1. Defining the model by inheriting torch.nn.Module class"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class LinearRegressionModel(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    # Using nn.Linear for creating model parameters\n","    self.linear_layer = nn.Linear(in_features=1, out_features=1)\n","    \n","  def forward(self, x : torch.Tensor)-> torch.Tensor:\n","    return self.linear_layer(x)"]},{"cell_type":"markdown","metadata":{},"source":["### 2.2. Creating a model instance, seeding the RNG for reproduciblity"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["torch.manual_seed(69)\n","model = LinearRegressionModel()\n","model.state_dict()\n","\n","# Checking the device accessing the model\n","next(model.parameters())\n","\n","# Sending the model to the available device (cuda preferred)\n","model.to(device)"]},{"cell_type":"markdown","metadata":{},"source":["### 2.3. Loss Function and Optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["loss_function = nn.L1Loss()\n","optimizer = torch.optim.SGD(params=model.parameters(), lr = 0.001)"]},{"cell_type":"markdown","metadata":{},"source":["### 2.4. Training Loop and Testing Loop"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train_model(tr_model = model, tr_features = X_train, tr_labels = Y_train, va_features = X_val, va_labels = Y_val, epochs = 10, opt = optimizer, loss = loss_function):\n","  # Training metrics\n","  epoch_c = []\n","  loss_val = []\n","  test_loss_val = []\n","  # Training\n","  time_start = time.time()\n","  for epoch in range(1, epochs+1):\n","    model.train()\n","    Y_preds_tr = tr_model(tr_features)\n","    tr_loss = loss(Y_preds_tr, tr_labels)\n","    opt.zero_grad()\n","    tr_loss.backward()\n","    opt.step()\n","    # Testing\n","    if epoch % 10 == 0:\n","      Y_preds_te = tr_model(va_features)\n","      te_loss = loss(Y_preds_te, va_labels)\n","      epoch_c.append(epoch)\n","      loss_val.append(tr_loss.item())\n","      test_loss_val.append(te_loss.item())\n","      print(f\"Epoch: {epoch} | Weight: {model.linear_layer.weight.item()} | Bias: {model.linear_layer.bias.item()}\\nLoss: {tr_loss} | Validation Loss: {te_loss}\")\n","  time_end = time.time()\n","  print(f\"Time Elapsed: {time_end - time_start}\")\n","  return epoch_c, loss_val, test_loss_val\n","  \n","  "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["epoch_count, loss_v, test_loss_v = train_model(epochs=670)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with torch.inference_mode():\n","  y_preds_te = model(X_test)\n","  plot_predictions(predictions=y_preds_te)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":2}
