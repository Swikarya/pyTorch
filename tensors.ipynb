{"cells":[{"cell_type":"markdown","metadata":{},"source":["# **Tensors**\n","**Tensors** are used to represent data in neural networks. Real world information are encoded into tensors for the computer and neural network to work on.\n","\n","The main advantage of using tensors is their ability to make use of hardware acceleration provided by GPUs and TPUs that are able to perform large sets of calculations efficiently by allowing for parallel processing."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Remember to include this line at the beginning of every .ipynb file to allow for the console to show all outputs from line evaluations and not just the last one\n","from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","# Setting up a device agnostic code\n","if torch.cuda.is_available:\n","    torch.set_default_device(\"cuda\")\n"]},{"cell_type":"markdown","metadata":{},"source":["After having imported all the necessary libraries, let's check for the version of PyTorch installed in the system."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["torch.__version__"]},{"cell_type":"markdown","metadata":{},"source":["## Creating Tensors\n","\n","All the data stored an utilized in pytorch are stored as tensors. PyTorch tensors are created using **torch.tensor()**\n","\n","Tensors are of several types and one of the classification is made on the basis of the rank of the tensor. They are:\n","\n","*   Rank 0 Tensors (No basis vectors utilized -- **Scalars**)\n","*   Rank 1 Tensors (One basis vector for each direction -- **Vectors**)\n","*   Rank 2 Tensors (Two basis vector for each direction)\n","*   Rank 3 Tensors (Three basis vectors for each direction)"]},{"cell_type":"markdown","metadata":{},"source":["### Scalars"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Creating scalars.\n","# Scalars are tensors of rank 0\n","SCALAR =  torch.tensor(7) # Returns a pytorch tensor with no \"autograd history\" --> look into autograd mechanics\n","torch.is_tensor(SCALAR) # Returns True if the passed object is a PyTorch tensor\n","SCALAR.ndim# Returns the number of dimensions of ndarray in python\n","SCALAR.shape# --> Look into it\n","SCALAR.item() # Returns the item in the scalar (tensor of rank 0) as a regular python integer."]},{"cell_type":"markdown","metadata":{},"source":["### Vectors"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Vectors are created similar to scalars\n","VECTOR = torch.tensor([7,7])\n","VECTOR.ndim\n","VECTOR.shape"]},{"cell_type":"markdown","metadata":{},"source":["### Matrices"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Matrices are created as\n","MATRIX = torch.tensor([[1,2,3], [3,4,5]])\n","MATRIX.ndim\n","MATRIX.shape\n","MATRIX[0]"]},{"cell_type":"markdown","metadata":{},"source":["### Tensors"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["TENSOR = torch.tensor([[[1,2,3],[4,5,6],[7,8,9]],[[10,11,12],[13,14,15],[16,17,18]],[[19,20,21],[22,23,24],[25,26,27]],[[28,29,30],[31,32,33],[34,35,36]]])\n","TENSOR.ndim\n","TENSOR.shape\n","TENSOR[1][2][1]"]},{"cell_type":"markdown","metadata":{},"source":["### Random Tensors\n","\n","Random tensors are useful because neural networks usually start with a random collection of data and then tune them to better fit the problem's solution.\n","Manually initializing tensors that may contain thousands of data is impracical"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Creating a random tensor of size (3,5,2)\n","random = torch.rand(5,3,4)\n","random[3][1][3].item()\n","random.ndim\n","random.shape\n","# Creating a tensor of shape similar to an image tensor\n","random_image = torch.rand(size = (244,244,3)) # Height, width and color channels\n","print(random_image.shape, random_image.ndim)"]},{"cell_type":"markdown","metadata":{},"source":["### Zeros and Ones\n","\n","The .zeros() method creates a tensor of required shape made up entirely of zeros. Tensors with only zeros and ones are used as masks to separate certain region of interest in an image. The .ones() method does the same thing but for ones for all elements."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ZEROS = torch.zeros(size = (5,10,10))\n","ONES = torch.ones(size = (5,3,4))\n","# print(ZEROS, ONES)\n","\n","# Notes that T1 * T2  where T1 and T2 are tensors performs a simple correspondent element multiplication. So,\n","# print(random * ZEROS)\n","\n","try:\n","  ONES*random\n","except RuntimeError:\n","  print(\"Mismatched Dimension\") # AS the code clearly explains"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ZEROS.dtype"]},{"cell_type":"markdown","metadata":{},"source":["### Range of tensors & tensor-like\n","\n",".arange() returns a rank 2 tensor with elements ranging from start (inclusive) to end (exclusive) with steps (1 by default)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["range = torch.arange(start=0, end=78) # .arange() is left inclusive and right exclusive\n","range"]},{"cell_type":"markdown","metadata":{},"source":[".zeros_like() returns a tensor of shape same as that of input but with each element zero .ones_like() works in a similar way but for ones and .rand_like() for random values"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tens_zeroes_like = torch.rand_like(input=random)\n","tens_zeroes_like"]},{"cell_type":"markdown","metadata":{},"source":["## Tensor datatypes\n","\n","Tensors in PyTorch by default store data in float_32 dtype unless explicitly initialized ie. .rand(), .zeroes(), .ones(), and .rand_like() return a tensor of dtype float_32. The major error points while coding with PyTorch are:\n","*  selection of datatypes \n","*  wrong tensor dimensions \n","*  tensors not on the right device\n","\n","While initializing tensors we can pass params like:\n","*  dtype: datatype of tensor (torch.floa32 / torch.float64)\n","*  device: which device is the tensor on or associated with GPU or CPU\n","*  requires_grad: if PyTorch should track the gradients of the tensor while it is computed on"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["float_32 = torch.tensor([3.0,6.0,9.0], dtype=None, device=None, requires_grad=False)\n","new = torch.rand_like(input=float_32, dtype=torch.float64)\n","new\n","float_32\n","float_32.dtype"]},{"cell_type":"markdown","metadata":{},"source":["### .dtype \n","\n","is the property of tensor that represents the type of data stored in the tensor"]},{"cell_type":"markdown","metadata":{},"source":["### Typecasting\n","\n","Explicit typecasting is done by .type() method"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["float_16 = float_32.type(torch.float16)\n","float_16.dtype"]},{"cell_type":"markdown","metadata":{},"source":["Now, we see that the result is implicitly (implies that the result must be of higher order) typecasted to be float32\n","\n","> Note that the .type() method returns the typecasted tensor and so the returned tensor has to be assigned to some other new tensor or the original tensor"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["result = float_16 * float_32\n","result.dtype"]},{"cell_type":"markdown","metadata":{},"source":["Tensor attributes can be fetched as:\n","*  datatype: tensor.dtype\n","*  device: tensor.device\n","*  shape: tensor.shape | can also use tensor.size(). While shape is a property, size is a method"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["result.dtype\n","result.shape\n","result.size()\n","result.device"]},{"cell_type":"markdown","metadata":{},"source":[".to() method can change the tensor attributes like device and dtype"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# result.to(device=\"cuda\", dtype=torch.float64)"]},{"cell_type":"markdown","metadata":{},"source":["## Tensor Manipulation\n","\n","Tensors can be operated on as:\n","* Addition\n","* Subtraction\n","* Multiplication (element-wise)\n","* Division\n","* Matrix multiplication"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test = torch.tensor([[[1,2,3],[4,5,6]]], dtype=torch.long)\n","\n","# Addition\n","test + 7\n","\n","# Multiplication\n","new_test = test * 4\n","\n","# Subtraction\n","test - 12\n","\n","# Division\n","test / 5"]},{"cell_type":"markdown","metadata":{},"source":["Tensors are multiplied in two ways:\n","\n","* Multiplication with a scalar (*)\n","* Matrix Multiplication (@)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Scalar multiplication\n","print(f\"{test} * {new_test} = {test * new_test}\")\n","\n","# Matrix multiplication\n","# torch.matmul(test, new_test)\n","\n","d1 = torch.tensor([[1,2,3],[4,5,6]])\n","d2 = torch.tensor([[1,2],[3,4]])\n","try:\n","  torch.matmul(d1,d2)\n","except Exception as exp:\n","  print(exp)"]},{"cell_type":"markdown","metadata":{},"source":["One of the most frequent errors faced while working with neural networks and writing deep learning code is the size mismatch of tensors that are being multiplied. \n","\n","The two important rules followed are:\n","\n","1. The inner dimensions must match:\n","  * `(3,2) @ (2,3)` will work\n","  * `(3,2) @ (3,2)` won't work\n","  * `(2,3) @ (3,2)` will work\n","  \n","2. The resulting matrix will have the shape of outer dimensions\n","\n",">Note: inner dimensions for (5,7) and (4,6) are (5,4) and outer dimensions are (7,6)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Transposing tensors\n","\n","tensor = torch.rand([3,3,2])\n","tensor\n","# tensor.T\n","# tensor.T is a deprecated feature and a better way to transpose matrices is by using the permute function as follows\n","tensor.permute(*torch.arange(tensor.ndim - 1, -1, -1))\n","# This is a rather intuitive way to transpose or reverse the dimensions "]},{"cell_type":"markdown","metadata":{},"source":["## Tensor Aggregation\n","\n","We can find the max, min, sum, avg of a tensor by using tensor aggregation methods."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["random_tensor = torch.rand([3,3,3]) * 100\n","random_tensor = random_tensor.type(torch.int16) # Refer to the note in explicit typecasting section\n","random_tensor\n","random_tensor.dtype"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Finding min\n","random_tensor.min()\n","\n","# Finding max\n","random_tensor.max()\n","\n","# Finding mean\n","try:\n","  random_tensor.mean()\n","except RuntimeError:\n","  print(\"Tensor must be of dtype float or complex and not integer\")\n","\n","# Finding sum\n","random_tensor.sum()\n","\n","# Finding the positional min\n","random_tensor.argmin()\n","\n","# Finding the positional max\n","random_tensor.argmax()"]},{"cell_type":"markdown","metadata":{},"source":["> Note that the .argmin() and .argmax() methods return the position of the min and max value assuming that the tensor is one dimensional ie. In a 3x3x3 tensor if the element of index `[1][2][1]` is min then the value returned by the method will be 16. We can see that the reasoning behind the result is that position is counted as 8 when we reach `[0][2][2]` from `[0][0][0]` and continue as 9 for `[1][0][0]`"]},{"cell_type":"markdown","metadata":{},"source":["## Reshaping, Stacking, Squeezing, and Unsqueezing Tensors\n","\n","* Reshape: reshape an input tensor to a desired shape\n","* View: return a view of an input tensor of certain shape but keep the same memory as the original tensor\n","* Stacking: stack multiple tensors on top of one another (**vstack**) or side-by-side (**hstack**)\n","* Squeeze: removes all `1` dimensions from a tensor\n","* Unsqueeze: adds a `1` dimension to a traget tensor\n","* Permute: return a view of the input tensor with its dimension permuted (**swapped**) in a certain way"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tensor_new = torch.arange(1.0,49.0)\n","tensor_new\n","\n","# Reshaping a tensor\n","try:\n","  reshaped_tensor = tensor_new.reshape([3,4,4])\n","except Exception as err:\n","  print(str(err) + \"Total elements: 48 N 50 O\")\n","  try:\n","    reshaped_tensor = tensor_new.reshape([4,13])\n","  except Exception as err:\n","      print(str(err) + \"Total elements: 52 N 50 O\")\n","      reshaped_tensor = tensor_new.reshape([5,5,2])\n","      reshaped_tensor\n","# reshaped_tensor = tensor_new.reshape([5,2,5])\n","# reshaped_tensor\n"]},{"cell_type":"markdown","metadata":{},"source":["> A key concept to note is that the .reshape() method can only reshape tensors into new tensors that are equivalent to the original i.e. the number of elements in them must be equal. We cannot exclude some elements from the new tensor nor can we create a tensor with empty positions."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Changing the view of a tensor \n","view_tensor = tensor_new.view(3,2,4,1,2)\n","view_tensor"]},{"cell_type":"markdown","metadata":{},"source":["One thing to note about the .view() method is that the tensor returned by it will share the same memory as the original tensor but the shape wiil be changed. Essentially, the changes made to the tensor returned will be reflected to the original tensor."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Manipulating the view_tensor\n","view_tensor *= 10.0\n","tensor_new\n","tensor_new /= 10.0\n","view_tensor"]},{"cell_type":"markdown","metadata":{},"source":["We see that the chages were applied to both view_tensor and tensor_new"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Stacking tensors\n","stacked_tensor = torch.stack([view_tensor, view_tensor, view_tensor], dim=4)\n","view_tensor\n","stacked_tensor\n","view_tensor.shape\n","stacked_tensor.shape"]},{"cell_type":"markdown","metadata":{},"source":["The .stack() method stacks tensors. It takes two arguments, a tensor list and `dim`. While the list is self explanatory one may find it hard to grasp what dim does. The dim argument specifies the index of dimension that will be added to the stacked tensor ie. for `dim = 0` the tensors wll be stacked one after other. say three tensors of dimension `[2,3]` were stacked so the dimension of the new tensor will be `[3,2,3]`. The same tensor will have dimension `[2,3,3]` for `dim = 2`.\n","> Play around in the code block above to better understand it.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Squeezing tensors\n","squeezed_tensor = view_tensor.squeeze()\n","squeezed_tensor\n","squeezed_tensor.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Unsqueezing tensors\n","unsqueezed_tensor = squeezed_tensor.unsqueeze(1)\n","squeezed_tensor\n","unsqueezed_tensor\n","squeezed_tensor.shape\n","unsqueezed_tensor.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Permuting tensors\n","permuted_tensor = torch.permute(squeezed_tensor, (1,3,2,0))\n","permuted_tensor\n","permuted_tensor.shape\n","\n","# Demonstrating that the permute method returns tensor in similar manner to view.\n","squeezed_tensor *= 10\n","permuted_tensor"]},{"cell_type":"markdown","metadata":{},"source":["The permute method reorganizes the input tensor in a specific order. To specify the order a tuple of the same length as the size tuple is to be passed where the index of the original size tuple are entered. Example: A tensor of size `(3,4,5,8)` is passed to `.permute(tensor, (2,3,0,1))`, the resulting tensor will have a size of `(5,8,3,4)`"]},{"cell_type":"markdown","metadata":{},"source":["## Indexing\n","\n","Indexing with PyTorch is similar to indexing with NumPy"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","tensor_test = torch.arange(1,10).reshape([1,3,3])\n","tensor_test"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Accesing [0] th element of tensor\n","tensor_test[0]\n","# We get the same tensor as the dimension of tensor_test is [1,3,3]\n","# Accessing the 2nd element of 1st row \n","tensor_test[0][0][1]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Using : for indexing\n","# : selects all the elements in a dimension or ing general a list\n","tensor_test[0,:,1]"]},{"cell_type":"markdown","metadata":{},"source":["- `tensor_test[0,:,1]` - This will select all elements from the second dimension (columns) of the first dimension (rows) at index 1 of the tensor. In other words, itâ€™s selecting the second column from all rows of the first matrix in the tensor.\n","\n","- `tensor_test[0][:][1]` - This is a chained indexing operation. The first indexing operation tensor_test[0] selects the first matrix in the tensor. The second indexing operation [:] selects all elements of this matrix. The third indexing operation [1] then selects the second element of the result. In this case, it will select the second row of the first matrix in the tensor.\n","\n","> So, the difference between tensor_test[0,:,1] and tensor_test[0][:][1] is that the former selects a column from a matrix in the tensor, while the latter selects a row from a matrix in the tensor.\n","\n","To fetch `3,6,9` from the tensor"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tensor_test[0,:,2]"]},{"cell_type":"markdown","metadata":{},"source":["## PyTorch and Numpy\n","\n","Numpy being a widely used numerical computation library has been integrated i Pyorch to allow for better array and tensor manipulation. We can interchange betwee a Numpy ndarray and a pytorch tensor as follows:\n","\n","- Pytorch -> Numpy: use `torch.Tensor.numpy()`\n","- Numpy -> pytorch: use `torch.from_numpy(ndarray)`"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["array = np.arange(1.0,9.0)\n","to_tensor = torch.from_numpy(array)\n","array, to_tensor, id(array), id(to_tensor)"]},{"cell_type":"markdown","metadata":{},"source":["> Note that the `.from_numpy()` method reflects the default dtype of float64 of numpy while PyTorch has a default dtype of float32. Also the tensor thus formeed will not refer to the same memory location and a new variable is being created upon calling it."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["id(array), id(to_tensor)\n","array = array * 2\n","id(array), id(to_tensor)\n","to_tensor\n"]},{"cell_type":"markdown","metadata":{},"source":["> Note that manipulating variables using `a += 1` and `a = a + 1` are different. The former changes the value in-place while the latter changes the value and creates a new memory instance of it. This is why if the array was manipulated using `+=` the tensor referring to its memory was also changed while using `a = a + n` results in the array changing but the tensor remaining the same."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Changing from tensor to numpy\n","tensor = torch.ones([7,3])\n","numpy_tensor = tensor.numpy()\n","tensor, numpy_tensor"]},{"cell_type":"markdown","metadata":{},"source":["## Reproducibility\n","\n","The ability to recreate results when a program runs for multiple instances is `reporducibility`. The ability to reproduce an outcome is in direct contrast to the ability to work with random initial conditions. \n","\n","Sometimes you want the program to behave in the same way even though it is meant to work with randomly generated values later on. For this, instead of changing the program for debugging and for applicaton we can `seed` the `RNG` such that the random number generator will always produce the same set of values for all program executions."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["torch.manual_seed(32)\n","random_tensor_one = torch.rand([3,4,5])\n","torch.manual_seed(32)\n","random_tensor_two = torch.rand([3,4,5])\n","random_tensor_one\n","random_tensor_two\n","random_tensor_one == random_tensor_two\n","\n","# Seeding the CUDA RNG for one GPU, in multi-GPU systems, to seed all GPUs at once use .seed_all()\n","torch.cuda.manual_seed(0)\n","rt = torch.rand([2,2,2], device = device)\n","rt\n","torch.cuda.manual_seed(0)\n","nrt = torch.rand([2,2,2], device = device)\n","nrt\n","rt == nrt\n"]},{"cell_type":"markdown","metadata":{},"source":["## Accessing a GPU and Device Agnostic Code\n","\n","Utilizing a GPU for ML operations siggnificantly reduces runtime once the models and training data scale up. \n","\n","> A device agnostic code is a program that utilizes the available resources in terms of both hardware and software without requiring any tweaking to fit the system where it is ran."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check if a GPU is available\n","torch.cuda.is_available()"]},{"cell_type":"markdown","metadata":{},"source":["### Putting tensors from CPU to GPU and vice versa\n","\n","PyTorch can utlize GPU to compute tensor operations faster while numpy can only acces the cpu. So, sometimes we have t transfer our cpu tensor to gpu and vice versa."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Creating a tensor defaults to it being made for CPU\n","tensor = torch.tensor([1,2,3])\n","tensor, tensor.device\n","\n","# Creating tensor on GPU if available\n","gpu_tensor = torch.tensor([1,2,3] , device = device)\n","gpu_tensor.device\n","\n","# Putting cpu tensor on th GPU\n","transferred = tensor.to(device)\n","transferred.device\n","\n","# Putting tensors to CPU\n","transferred_again = gpu_tensor.to(\"cpu\")\n","transferred_again.device\n","gpu_tensor"]},{"cell_type":"markdown","metadata":{},"source":["Some operations are not compatible for the tensors due to the device they are accessing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["try:\n","  gpu_tensor.numpy()\n","except Exception as error:\n","  print(\"Error:\",error)\n","  # gpu_tensor.to(\"cpu\").numpy()\n","  # A better way to do that is by calling the .cpu() method on the tensor\n","  gpu_tensor.cpu().numpy()\n","  gpu_tensor\n","  # Remember to assign the new tensors to something else ot to itself"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":2}
